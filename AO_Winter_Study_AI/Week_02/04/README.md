# [2주차] 학습 정리

## 1. Perceptron (퍼셉트론)
> **인공신경망의 가장 기초적인 단위**

### 1) 정의 및 동작 원리
* **정의:** 뉴런의 동작 과정을 모방하여 만든 수학적 모델.
* **동작 방식:**
  1. 입력값($x$)과 가중치($w$)를 곱하고 모두 더함 (가중합).
  2. 여기에 **편향(Bias)**을 더해 0이 되지 않도록 조정.
  3. 이 값이 **활성화 함수**를 거쳐 임계치(보통 0)보다 크면 **1**, 아니면 **0**을 출력.

### 2) 특징 및 한계
* **특징:** 결과값으로 보통 0 또는 1을 출력하는 **선형 모델(Linear Classifier)**.
* **한계 (XOR 문제):**
  * 직선 하나($ax+by+c=0$)만 그을 수 있기 때문에, **AND, OR** 문제는 해결 가능.
  * 하지만 대각선으로 데이터가 나뉘는 **XOR 문제**는 직선 하나로 절대 나눌 수 없음 (비선형 분리 불가).

---

## 2. MLP (Multi-Layer Perceptron)
> **퍼셉트론의 한계를 극복하기 위해 '은닉층'을 추가한 구조**

### 1) 구조
* **입력층 (Input Layer):** 데이터 수신.
* **은닉층 (Hidden Layer):** 입력과 출력 사이에 존재하며, **복잡한 패턴을 학습하는 핵심**.
* **출력층 (Output Layer):** 최종 결과 출력.

### 2) 핵심 포인트
* **입력 공간의 왜곡 (Warping):** 은닉층이 좌표 평면을 구부리고 왜곡하여, 선형 분리가 불가능했던 XOR 같은 문제도 해결할 수 있게 만듦.
* **🚨 활성화 함수의 중요성 (비선형성):**
  * 은닉층이 아무리 많아도 **활성화 함수(비선형)**가 없으면, 수학적으로는 **단 하나의 층(단층 퍼셉트론)**과 똑같아짐.
  * 즉, 신경망을 깊게 쌓는 의미를 갖게 하려면 비선형성을 부여하는 활성화 함수가 필수.

---

## 3. FeedForward (순전파)
* **정의:** 정보가 **입력층 → 은닉층 → 출력층**의 한 방향으로만 흐르는 구조 (루프 없음).
* **처리 과정:**
  1. **입력:** 데이터 받기
  2. **선형 변환:** 가중치($w$) 곱하기 + 편향($b$) 더하기
  3. **비선형 변환:** 활성화 함수 통과
  4. **출력:** 최종 결과 도출
* **의의:** 신경망은 이 과정을 통해 입력과 출력 사이의 복잡한 함수를 데이터로부터 근사함.

---

## 4. Backpropagation (역전파) & Loss
> **학습의 핵심: 오차를 줄이는 방향으로 가중치 수정**

### 1) 주요 개념
* **Loss (손실 함수):**
  * 모델의 예측이 정답에서 **얼마나 틀렸는지**를 수치로 나타낸 값.
  * 단순히 맞다/틀리다가 아니라 '오차의 크기'를 알아야 학습 가능.
* **Gradient (기울기):**
  * 손실(Loss)을 줄이기 위해 가중치를 **어느 방향으로, 얼만큼** 조절해야 하는지 알려주는 나침반.

### 2) 동작 원리 (연쇄 법칙)
* **정의:** 출력층에서 발생한 오차를 **입력층 방향(역방향)**으로 전파하며 각 가중치를 수정하는 알고리즘.
* **연쇄 법칙 (Chain Rule):**
  * 층이 깊어질수록 미분이 복잡해지는데, 이를 단계별로 분해하여 효율적으로 계산하는 수학적 원리.
  * 이를 통해 가장 앞단에 있는 가중치가 오차에 얼마나 기여했는지(기여도)를 정확히 계산 가능.